{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[10, 20, 30],\n",
    "                  [20, 30, 40],\n",
    "                  [30, 40, 50],\n",
    "                  [40, 50, 60],\n",
    "                  [50, 60, 70]], dtype=torch.float)\n",
    "\n",
    "y = torch.tensor([[40],\n",
    "                  [50],\n",
    "                  [60],\n",
    "                  [70],\n",
    "                  [80]], dtype=torch.float)\n",
    "\n",
    "# MLP (13 epochs)\n",
    "model = nn.Sequential(nn.Linear(3, 100),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(100, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[10, 20, 30],\n",
    "                  [20, 30, 40],\n",
    "                  [30, 40, 50],\n",
    "                  [40, 50, 60],\n",
    "                  [50, 60, 70]], dtype=torch.float).view(-1, 1, 3)\n",
    "\n",
    "y = torch.tensor([[40],\n",
    "                  [50],\n",
    "                  [60],\n",
    "                  [70],\n",
    "                  [80]], dtype=torch.float)\n",
    "\n",
    "# CNN 1D (10 epochs)\n",
    "model = nn.Sequential(nn.Conv1d(1, 64, 2),\n",
    "                      nn.ReLU(),\n",
    "                      nn.MaxPool1d(2),\n",
    "                      nn.Flatten(),\n",
    "                      nn.Linear(64, 50),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(50, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[10, 20, 30],\n",
    "                  [20, 30, 40],\n",
    "                  [30, 40, 50],\n",
    "                  [40, 50, 60],\n",
    "                  [50, 60, 70]], dtype=torch.float).view(-1, 3, 1)\n",
    "\n",
    "y = torch.tensor([[40],\n",
    "                  [50],\n",
    "                  [60],\n",
    "                  [70],\n",
    "                  [80]], dtype=torch.float)\n",
    "\n",
    "# LSTM (20 epochs)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(1, 50, batch_first=True)\n",
    "\n",
    "        self.tail = nn.Sequential(nn.ReLU(), nn.Linear(50, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "\n",
    "        # inverse the activation function applied by CuDNN LSTM\n",
    "        out = 0.5 * (torch.log(1 + out) / (1 - out))\n",
    "\n",
    "        return self.tail(out[:, -1])\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[10, 20, 30, 40],\n",
    "                  [20, 30, 40, 50],\n",
    "                  [30, 40, 50, 60],\n",
    "                  [40, 50, 60, 70],\n",
    "                  [50, 60, 70, 80]], dtype=torch.float).view(-1, 2, 2, 1) # .rename('N', 'S', 'L', 'C')\n",
    "\n",
    "y = torch.tensor([[50],\n",
    "                  [60],\n",
    "                  [70],\n",
    "                  [80],\n",
    "                  [90]], dtype=torch.float)\n",
    "\n",
    "# CNN-LSTM (100 epochs)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Sequential(nn.Conv1d(1, 64, 1),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.MaxPool1d(2),\n",
    "                                  nn.Flatten())\n",
    "\n",
    "        self.lstm = nn.LSTM(64, 50, batch_first=True)\n",
    "\n",
    "        self.tail = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x, debug=False):\n",
    "        N, S, L, C = x.shape\n",
    "        if debug: print(x.shape)\n",
    "\n",
    "        x = x.view(N * S, L, C)\n",
    "        if debug: print(x.shape)\n",
    "\n",
    "        x = x.rename('N', 'L', 'C')\n",
    "        x = x.transpose('L', 'C')\n",
    "        x = x.rename(None)\n",
    "        if debug: print(x.shape)\n",
    "\n",
    "        x = self.head(x)\n",
    "        if debug: print(x.shape)\n",
    "\n",
    "        x = x.view(N, S, -1)\n",
    "        if debug: print(x.shape)\n",
    "\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        x = x.rename('N', 'S', 'F')\n",
    "        if debug: print(x.shape)\n",
    "\n",
    "        x = x.rename(None)\n",
    "        x = x[:, -1]\n",
    "\n",
    "        # x = 0.5 * (torch.log(1 + x) / (1 - x))\n",
    "\n",
    "        return self.tail(x)\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[10, 20, 30],\n",
    "                  [20, 30, 40],\n",
    "                  [30, 40, 50],\n",
    "                  [40, 50, 60],\n",
    "                  [50, 60, 70]], dtype=torch.float).view(-1, 3, 1)\n",
    "\n",
    "y = torch.tensor([[40, 50],\n",
    "                  [50, 60],\n",
    "                  [60, 70],\n",
    "                  [70, 80],\n",
    "                  [80, 90]], dtype=torch.float).view(-1, 2, 1)\n",
    "\n",
    "# LSTM (20 epochs)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.LSTM(  1, 100, batch_first=True)\n",
    "        self.decoder = nn.LSTM(100, 100, batch_first=True)\n",
    "\n",
    "        self.tail = nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, x, debug=False):\n",
    "        x, _ = self.encoder(x)\n",
    "        if debug: print(x.shape)\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = x[:, -1:]\n",
    "        if debug: print(x.shape)\n",
    "\n",
    "        x = x.repeat(1, 2, 1)\n",
    "        if debug: print(x.shape)\n",
    "\n",
    "        x, _ = self.decoder(x)\n",
    "        if debug: print(x.shape)\n",
    "\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.tail(x)\n",
    "        if debug: print(x.shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(X, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([param.numel() for param in model.parameters() if param.requires_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(X[  :-1], y[  :-1])\n",
    "valid_dataset = TensorDataset(X[-1:  ], y[-1:  ])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, shuffle=True)\n",
    "\n",
    "assert len(train_loader) == 4\n",
    "assert len(valid_loader) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "n_epochs = 100\n",
    "valid_loss_min = np.Inf\n",
    "writer = SummaryWriter('logs/cnn-lstm')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    cum_train_loss = 0.\n",
    "    cum_valid_loss = 0.\n",
    "\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        # ZERO PREVIOUS GRADS\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model.forward(X)\n",
    "        assert y.shape == y_pred.shape\n",
    "        loss = criterion(y_pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cum_train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in valid_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            y_pred = model.forward(X)\n",
    "            assert y.shape == y_pred.shape\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            cum_valid_loss += loss.item()\n",
    "\n",
    "    train_loss = cum_train_loss / len(train_loader)\n",
    "    valid_loss = cum_valid_loss / len(valid_loader)\n",
    "\n",
    "    print(f'Train loss: {train_loss:.5} - Validation loss: {valid_loss:.5} - {y_pred}')\n",
    "\n",
    "#     if valid_loss < valid_loss_min:\n",
    "#         print('Validation loss decreased: %.5f => %.5f | Saving model...' % (valid_loss_min, valid_loss))\n",
    "#         torch.save(model.state_dict(), 'model.pt')\n",
    "#         valid_loss_min = valid_loss\n",
    "\n",
    "    writer.add_scalars('loss', dict(train_loss=train_loss, valid_loss=valid_loss), epoch)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
